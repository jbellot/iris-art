---
phase: 03-ai-processing-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/models/processing_job.py
  - backend/app/schemas/processing.py
  - backend/app/services/processing.py
  - backend/app/api/routes/processing.py
  - backend/app/workers/models/__init__.py
  - backend/app/workers/models/model_cache.py
  - backend/app/workers/models/segmentation_model.py
  - backend/app/workers/models/reflection_model.py
  - backend/app/workers/models/enhancement_model.py
  - backend/app/workers/tasks/processing.py
  - backend/app/workers/celery_app.py
  - backend/app/main.py
  - backend/requirements.txt
  - backend/docker-compose.yml
  - backend/alembic/versions/
autonomous: true

must_haves:
  truths:
    - "User can submit a photo for AI processing and receives a job ID"
    - "Celery worker executes segmentation, reflection removal, and enhancement in sequence"
    - "Processed result image is saved to S3 and job record updated with result URL"
    - "Batch submission queues multiple photos for processing"
    - "Failed processing jobs have classified error types (quality vs transient)"
  artifacts:
    - path: "backend/app/models/processing_job.py"
      provides: "ProcessingJob SQLAlchemy model with status, step tracking, error classification"
      contains: "class ProcessingJob"
    - path: "backend/app/workers/models/model_cache.py"
      provides: "Singleton model cache for AI models loaded once at worker startup"
      contains: "class ModelCache"
    - path: "backend/app/workers/tasks/processing.py"
      provides: "Celery pipeline task orchestrating segmentation, reflection removal, enhancement"
      contains: "process_iris_pipeline"
    - path: "backend/app/api/routes/processing.py"
      provides: "Processing job submission and status API endpoints"
      exports: ["router"]
    - path: "backend/app/schemas/processing.py"
      provides: "Pydantic schemas for job submission, status, and results"
      contains: "JobSubmitRequest"
  key_links:
    - from: "backend/app/api/routes/processing.py"
      to: "backend/app/workers/tasks/processing.py"
      via: "Celery apply_async"
      pattern: "process_iris_pipeline\\.apply_async"
    - from: "backend/app/workers/tasks/processing.py"
      to: "backend/app/workers/models/model_cache.py"
      via: "ModelCache singleton"
      pattern: "ModelCache\\.get_"
    - from: "backend/app/workers/tasks/processing.py"
      to: "backend/app/storage/s3.py"
      via: "S3 download and upload"
      pattern: "s3_client\\.(download_file|upload_file)"
---

<objective>
Build the complete backend AI processing pipeline: database model for tracking processing jobs, AI model infrastructure with singleton caching, Celery tasks for iris segmentation/reflection removal/enhancement, and API endpoints for submitting and querying jobs.

Purpose: This is the backend engine that makes Phase 3 work. Users need to submit photos for AI processing and get results back. Without this, there is nothing to show progress on or display results from.

Output: Working backend that accepts photo processing requests, runs them through AI pipeline via Celery, saves results to S3, and exposes job status via REST API.
</objective>

<execution_context>
@/home/jbellot/.claude/get-shit-done/workflows/execute-plan.md
@/home/jbellot/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ai-processing-pipeline/03-RESEARCH.md
@.planning/phases/01-foundation-and-privacy-architecture/01-01-SUMMARY.md
@.planning/phases/02-camera-capture-and-image-upload/02-01-SUMMARY.md

Key existing infrastructure:
- Celery app: backend/app/workers/celery_app.py (autodiscovers tasks from app.workers.tasks)
- S3 client: backend/app/storage/s3.py (upload_file, download_file, generate_presigned_url)
- Photo model: backend/app/models/photo.py (has s3_key, user_id, upload_status)
- API deps: backend/app/api/deps.py (get_session, get_current_active_user)
- Config: backend/app/core/config.py (Pydantic BaseSettings)
- Docker Compose: backend/docker-compose.yml (6 services)
- Requirements: backend/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: ProcessingJob model, schemas, service, and migration</name>
  <files>
    backend/app/models/processing_job.py
    backend/app/schemas/processing.py
    backend/app/services/processing.py
    backend/app/models/__init__.py
    backend/alembic/versions/
    backend/requirements.txt
  </files>
  <action>
    **1. Update requirements.txt** - Add AI/ML dependencies:
    ```
    torch>=2.0.0
    torchvision>=0.19.0
    onnxruntime>=1.20.0
    opencv-python-headless>=4.10.0
    Pillow>=11.0
    numpy>=2.0
    scikit-image>=0.24.0
    ```
    Keep all existing dependencies. These are needed for the AI pipeline tasks.

    **2. Create ProcessingJob model** at `backend/app/models/processing_job.py`:
    - UUID primary key (same pattern as Photo model)
    - `user_id` FK to users table with CASCADE delete
    - `photo_id` FK to photos table with CASCADE delete
    - `status` String: pending, processing, completed, failed (default: pending)
    - `current_step` String nullable: tracks current pipeline step (segmenting, removing_reflections, enhancing, saving)
    - `progress` Integer default 0: overall progress percentage (0-100)
    - `error_type` String nullable: quality_issue, transient_error, server_error
    - `error_message` String nullable: user-friendly error message
    - `suggestion` String nullable: actionable suggestion for user
    - `attempt_count` Integer default 0: number of processing attempts
    - `result_s3_key` String nullable: S3 key of final processed image
    - `mask_s3_key` String nullable: S3 key of segmentation mask
    - `processing_time_ms` Integer nullable: total processing duration in milliseconds
    - `result_width` Integer nullable, `result_height` Integer nullable
    - `quality_score` Float nullable: output quality metric (0-1)
    - `celery_task_id` String nullable: Celery task ID for status polling
    - `created_at` DateTime(timezone=True) server_default=func.now()
    - `updated_at` DateTime(timezone=True) server_default=func.now(), onupdate=func.now()
    - Relationship to User (back_populates), relationship to Photo

    **3. Create Pydantic schemas** at `backend/app/schemas/processing.py`:
    - `JobSubmitRequest`: photo_id (UUID)
    - `BatchJobSubmitRequest`: photo_ids (list[UUID], max 10)
    - `JobResponse`: job_id, status, current_step, progress, created_at, websocket_url
    - `JobStatusResponse`: extends JobResponse with error_type, error_message, suggestion, attempt_count, result_url (presigned), original_url (presigned), processing_time_ms, result_width, result_height
    - `BatchJobResponse`: jobs (list[JobResponse])

    **4. Create processing service** at `backend/app/services/processing.py`:
    - `create_processing_job(db, user_id, photo_id) -> ProcessingJob`: Creates job record, validates photo exists and belongs to user, returns job
    - `get_job(db, user_id, job_id) -> ProcessingJob | None`: Get job by ID (user-scoped)
    - `get_jobs_for_photo(db, user_id, photo_id) -> list[ProcessingJob]`: Get all jobs for a photo
    - `update_job_status(db, job_id, status, **kwargs)`: Update job fields (step, progress, error, result)
    - `get_user_jobs(db, user_id, page, page_size) -> tuple[list[ProcessingJob], int]`: Paginated user jobs
    - `generate_job_response_with_urls(job) -> JobStatusResponse`: Generate response with presigned URLs for result and original photo

    **5. Create Alembic migration** for processing_jobs table:
    Run: `cd backend && alembic revision --autogenerate -m "add_processing_job_model"`
    Then run: `cd backend && alembic upgrade head`

    **Important patterns to follow:**
    - Use same SQLAlchemy 2.0 async patterns as Photo model (Mapped, mapped_column)
    - Use UUID(as_uuid=True) for UUIDs
    - Use DateTime(timezone=True) for timestamps
    - Import Base from app.core.db
    - Follow existing schema patterns from backend/app/schemas/photo.py
  </action>
  <verify>
    cd backend && python -c "from app.models.processing_job import ProcessingJob; print('Model OK')"
    cd backend && python -c "from app.schemas.processing import JobSubmitRequest, JobResponse, JobStatusResponse; print('Schemas OK')"
    cd backend && python -c "from app.services.processing import create_processing_job; print('Service OK')"
    cd backend && alembic upgrade head  # Migration applies cleanly
  </verify>
  <done>
    ProcessingJob model exists with all fields. Schemas validate job request/response. Service provides CRUD operations. Migration creates processing_jobs table in PostgreSQL.
  </done>
</task>

<task type="auto">
  <name>Task 2: AI model infrastructure, Celery pipeline tasks, processing API, and priority queues</name>
  <files>
    backend/app/workers/models/__init__.py
    backend/app/workers/models/model_cache.py
    backend/app/workers/models/segmentation_model.py
    backend/app/workers/models/reflection_model.py
    backend/app/workers/models/enhancement_model.py
    backend/app/workers/tasks/processing.py
    backend/app/workers/celery_app.py
    backend/app/api/routes/processing.py
    backend/app/main.py
    backend/docker-compose.yml
  </files>
  <action>
    **1. Create model cache** at `backend/app/workers/models/model_cache.py`:
    - Singleton `ModelCache` class with class-level variables for each model
    - `get_segmentation_model()`: Returns ONNX Runtime InferenceSession (lazy load on first call)
      - Model path: `backend/app/workers/models/weights/unet_iris_segmentation.onnx`
      - Providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']
      - If model file not found, log warning and return None (graceful degradation for dev without models)
    - `get_enhancement_model()`: Returns Real-ESRGAN enhancer (lazy load on first call)
      - Model path: `backend/app/workers/models/weights/RealESRGAN_x4plus.pth`
      - If model file not found, log warning and return None
    - `get_reflection_model()`: Returns reflection removal model (lazy load)
      - For MVP: return None (use OpenCV inpainting fallback per research recommendation)
    - Each getter logs which provider/device is being used on first load
    - Add `backend/app/workers/models/__init__.py` (empty)

    **2. Create segmentation model wrapper** at `backend/app/workers/models/segmentation_model.py`:
    - `segment_iris(image: np.ndarray) -> tuple[np.ndarray, np.ndarray]`: Returns (segmented_image, mask)
    - If ONNX model available: resize to 512x512, normalize to [0,1], run inference, threshold at 0.5, unpad, resize mask back to original dimensions
    - If ONNX model not available (dev mode): create a simulated circular mask centered on image (for testing pipeline without real model)
    - Apply mask to original image using cv2.bitwise_and
    - Validate: mask must cover at least 5% of image area, else raise ValueError("Iris not detected clearly")
    - Return both the masked image and the binary mask

    **3. Create reflection removal wrapper** at `backend/app/workers/models/reflection_model.py`:
    - `remove_reflections(image: np.ndarray, mask: np.ndarray) -> np.ndarray`: Returns image with reflections removed
    - For MVP: Use OpenCV inpainting (cv2.inpaint with TELEA or NS method) per research recommendation
    - Detect specular highlights within the mask region (high-brightness pixels > 240 in grayscale)
    - Create inpainting mask from highlight regions
    - Apply cv2.inpaint with radius=3 using INPAINT_TELEA
    - If no highlights detected, return image unchanged
    - This is a functional placeholder; can be replaced with LapCAT transformer model later

    **4. Create enhancement model wrapper** at `backend/app/workers/models/enhancement_model.py`:
    - `enhance_iris(image: np.ndarray, scale: int = 4) -> np.ndarray`: Returns enhanced image
    - If Real-ESRGAN model available: use RealESRGANer with tile=400, tile_pad=10, half=True for GPU
    - If model not available (dev mode): use cv2.resize with INTER_LANCZOS4 as fallback upscaling
    - Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) for color correction before upscaling
    - Return enhanced image

    **5. Create pipeline Celery task** at `backend/app/workers/tasks/processing.py`:
    - Create `RetryableProcessingTask(Task)` base class:
      - autoretry_for = (ConnectionError, TimeoutError, RuntimeError)
      - retry_backoff = True, retry_backoff_max = 600, retry_jitter = True
      - max_retries = 1 (per user decision: auto-retry ONCE on transient failures)
    - `process_iris_pipeline(self, job_id: str, photo_id: str, user_id: str)`:
      - This is the main orchestrator task (single Celery task, not chained)
      - Uses synchronous DB session (Celery workers are sync) -- create a sync session maker or use synchronous SQLAlchemy
      - **Step 1 (0-10%):** Load original image from S3 using s3_client.download_file
        - Update job: current_step='loading', progress=10
        - update_state with PROGRESS meta for WebSocket polling
      - **Step 2 (10-40%):** Run segmentation
        - Update job: current_step='segmenting', progress=20
        - Call segment_iris(image) -> (segmented, mask)
        - Update progress to 40 on completion
      - **Step 3 (40-60%):** Run reflection removal
        - Update job: current_step='removing_reflections', progress=50
        - Call remove_reflections(segmented, mask)
        - Update progress to 60
      - **Step 4 (60-90%):** Run enhancement
        - Update job: current_step='enhancing', progress=70
        - Call enhance_iris(reflection_removed)
        - Update progress to 90
      - **Step 5 (90-100%):** Save results to S3
        - Update job: current_step='saving', progress=95
        - Save processed image: `processed/{user_id}/{job_id}.jpg`
        - Save mask: `processed/{user_id}/{job_id}_mask.png`
        - Update job: status='completed', result_s3_key, mask_s3_key, result_width, result_height, processing_time_ms, progress=100
      - **Error handling:**
        - ValueError (quality issues): Set error_type='quality_issue', error_message=str(e), suggestion='Try capturing a new photo in better lighting with your eye centered.', status='failed'
        - ConnectionError/TimeoutError: Let autoretry handle it. On final failure, set error_type='transient_error', error_message='Processing timed out. Please try again.', status='failed'
        - Other exceptions: Set error_type='server_error', error_message='Something went wrong. Please try again later.', status='failed'
      - Track start_time at beginning, calculate processing_time_ms on completion
      - For DB updates within Celery task: use a sync session factory. Create `get_sync_session()` in app.core.db that returns a regular (non-async) Session. Import engine URL without +asyncpg (use psycopg2 or raw postgresql://).

    **IMPORTANT for sync DB in Celery:** Celery workers run synchronously. Create a sync SQLAlchemy engine and session in `backend/app/core/db.py`:
    ```python
    from sqlalchemy import create_engine
    from sqlalchemy.orm import Session, sessionmaker

    # Sync engine for Celery workers (replace asyncpg with psycopg2)
    SYNC_DATABASE_URL = settings.DATABASE_URL.replace("+asyncpg", "")
    sync_engine = create_engine(SYNC_DATABASE_URL)
    sync_session_maker = sessionmaker(bind=sync_engine, expire_on_commit=False)
    ```
    Add `psycopg2-binary` to requirements.txt for sync PostgreSQL access.

    **6. Update Celery app** at `backend/app/workers/celery_app.py`:
    - Add priority queue configuration per research Pattern 3:
      ```python
      from kombu import Queue
      celery_app.conf.update(
          task_queues=(
              Queue('high_priority', routing_key='high'),
              Queue('default', routing_key='default'),
          ),
          task_routes={
              'app.workers.tasks.processing.process_iris_pipeline': {'queue': 'high_priority'},
          },
          task_default_queue='default',
      )
      ```
    - Keep existing autodiscover_tasks config
    - Keep existing time limits (30 min task_time_limit is appropriate for AI processing)

    **7. Create processing API endpoints** at `backend/app/api/routes/processing.py`:
    - `POST /api/v1/processing/submit` (JobSubmitRequest -> JobResponse):
      - Validate photo exists and belongs to user
      - Create ProcessingJob via service
      - Submit process_iris_pipeline.apply_async(args=[str(job.id), str(photo_id), str(user_id)], task_id=str(job.id), queue='high_priority')
      - Return JobResponse with websocket_url = f'/ws/jobs/{job.id}'
    - `POST /api/v1/processing/batch` (BatchJobSubmitRequest -> BatchJobResponse):
      - Max 10 photos per batch
      - Create a job for each photo, submit each to Celery with descending priority (first=9, last=0)
      - Return list of JobResponse
    - `GET /api/v1/processing/jobs/{job_id}` (-> JobStatusResponse):
      - Return full job status with presigned URLs for result and original photo
    - `GET /api/v1/processing/jobs` (-> paginated list of JobStatusResponse):
      - Return user's processing jobs, newest first
    - `POST /api/v1/processing/jobs/{job_id}/reprocess` (-> JobResponse):
      - Validate original job exists and belongs to user
      - Create new ProcessingJob for same photo
      - Submit to Celery
      - Return new JobResponse

    **8. Register processing router** in `backend/app/main.py`:
    - Import and include processing router (same pattern as photos router)

    **9. Update Docker Compose** at `backend/docker-compose.yml`:
    - Update celery_worker command to consume both queues: `celery -A app.workers.celery_app worker -Q high_priority,default --loglevel=info --prefetch-multiplier=1`
    - Add `--prefetch-multiplier=1` to ensure AI tasks don't queue up in worker memory
    - Add a volume mount for model weights: `./app/workers/models/weights:/code/app/workers/models/weights` (so models can be added without rebuild)
    - Create empty `backend/app/workers/models/weights/.gitkeep` file for the weights directory

    **10. Create weights directory** at `backend/app/workers/models/weights/`:
    - Create `.gitkeep` to track directory in git
    - Add `*.onnx`, `*.pth`, `*.pt` to `.gitignore` (model weights should not be committed)
  </action>
  <verify>
    cd backend && python -c "from app.workers.models.model_cache import ModelCache; print('ModelCache OK')"
    cd backend && python -c "from app.workers.tasks.processing import process_iris_pipeline; print('Pipeline task OK')"
    cd backend && python -c "from app.api.routes.processing import router; print('Processing router OK')"
    cd backend && python -c "from app.workers.models.segmentation_model import segment_iris; print('Segmentation OK')"
    cd backend && python -c "from app.workers.models.reflection_model import remove_reflections; print('Reflection OK')"
    cd backend && python -c "from app.workers.models.enhancement_model import enhance_iris; print('Enhancement OK')"
    grep "processing" backend/app/main.py  # Router registered
    grep "high_priority" backend/app/workers/celery_app.py  # Priority queue configured
    grep "high_priority" backend/docker-compose.yml  # Worker consumes priority queue
  </verify>
  <done>
    AI model infrastructure loads models once at worker startup (or uses dev fallbacks). Celery pipeline task processes photos through segmentation -> reflection removal -> enhancement -> S3 save. Processing API accepts job submissions, returns status, supports batch and reprocess. Priority queues ensure user-initiated jobs run first. Docker Compose worker configured for AI workload.
  </done>
</task>

</tasks>

<verification>
1. ProcessingJob table exists in database (alembic upgrade succeeds)
2. Python imports work for all new modules (models, schemas, services, API, tasks)
3. Processing router registered at /api/v1/processing/*
4. Celery worker can discover processing tasks
5. Priority queue configuration present in celery_app.py
6. Docker Compose worker command includes both queues
7. Model cache gracefully handles missing model weights (dev mode)
</verification>

<success_criteria>
- Backend accepts POST /api/v1/processing/submit with a photo_id and returns a job_id
- Celery task process_iris_pipeline is discoverable and has retry configuration
- AI model wrappers have dev-mode fallbacks (simulated segmentation, OpenCV fallback enhancement)
- ProcessingJob model tracks status, step, progress, error classification, and result S3 keys
- Batch submission endpoint accepts up to 10 photo_ids
- Reprocess endpoint creates new job for existing photo
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-processing-pipeline/03-01-SUMMARY.md`
</output>
