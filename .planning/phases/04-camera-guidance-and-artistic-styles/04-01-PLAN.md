---
phase: 04-camera-guidance-and-artistic-styles
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - mobile/ios/FrameProcessors/IrisDetectionPlugin.swift
  - mobile/ios/FrameProcessors/IrisDetectionPlugin.m
  - mobile/ios/FrameProcessors/BlurDetectionPlugin.swift
  - mobile/ios/FrameProcessors/BlurDetectionPlugin.m
  - mobile/ios/FrameProcessors/LightingAnalysisPlugin.swift
  - mobile/ios/FrameProcessors/LightingAnalysisPlugin.m
  - mobile/android/app/src/main/java/com/irisart/frameprocessors/IrisDetectionPlugin.kt
  - mobile/android/app/src/main/java/com/irisart/frameprocessors/BlurDetectionPlugin.kt
  - mobile/android/app/src/main/java/com/irisart/frameprocessors/LightingAnalysisPlugin.kt
  - mobile/android/app/src/main/java/com/irisart/frameprocessors/FrameProcessorPackage.kt
  - mobile/src/frameProcessors/irisDetection.ts
  - mobile/src/frameProcessors/blurDetection.ts
  - mobile/src/frameProcessors/lightingAnalysis.ts
  - mobile/src/hooks/useIrisDetection.ts
  - mobile/src/components/Camera/CameraGuidanceOverlay.tsx
  - mobile/src/components/Camera/IrisAlignmentGuide.tsx
  - mobile/src/components/Camera/FocusQualityIndicator.tsx
  - mobile/src/components/Camera/LightingIndicator.tsx
  - mobile/src/components/Camera/BurstCaptureButton.tsx
  - mobile/src/screens/Camera/CameraScreen.tsx
  - mobile/src/navigation/types.ts
autonomous: true

must_haves:
  truths:
    - "User sees real-time overlay showing iris detection circle, alignment feedback, and distance hint while capturing"
    - "User sees focus quality indicator (sharp/blurry) that updates in real-time as they hold the camera"
    - "User sees lighting indicator (too dark/bright/good) updating live during capture"
    - "User taps shutter once and app captures 3 frames, auto-selects the sharpest/best-aligned frame"
  artifacts:
    - path: "mobile/ios/FrameProcessors/IrisDetectionPlugin.swift"
      provides: "Native iOS iris detection using CoreImage face/eye detection"
    - path: "mobile/android/app/src/main/java/com/irisart/frameprocessors/IrisDetectionPlugin.kt"
      provides: "Native Android iris detection using ML Kit face detection"
    - path: "mobile/src/components/Camera/CameraGuidanceOverlay.tsx"
      provides: "Composite overlay with alignment, focus, lighting indicators"
    - path: "mobile/src/components/Camera/BurstCaptureButton.tsx"
      provides: "Burst capture button with 3-frame capture and best-frame selection"
    - path: "mobile/src/hooks/useIrisDetection.ts"
      provides: "Hook combining frame processor results into guidance state"
  key_links:
    - from: "mobile/src/frameProcessors/irisDetection.ts"
      to: "IrisDetectionPlugin (native)"
      via: "VisionCameraProxy.initFrameProcessorPlugin"
      pattern: "initFrameProcessorPlugin.*detectIris"
    - from: "mobile/src/components/Camera/CameraGuidanceOverlay.tsx"
      to: "mobile/src/hooks/useIrisDetection.ts"
      via: "useIrisDetection hook provides guidance state"
      pattern: "useIrisDetection"
    - from: "mobile/src/screens/Camera/CameraScreen.tsx"
      to: "mobile/src/components/Camera/CameraGuidanceOverlay.tsx"
      via: "Camera screen renders guidance overlay with frame processor"
      pattern: "CameraGuidanceOverlay"
---

<objective>
Add real-time AI camera guidance with on-device iris detection, focus quality, and lighting feedback -- plus multi-frame burst capture with automatic best-frame selection.

Purpose: Users get real-time coaching while capturing their iris, dramatically improving photo quality and reducing failed captures. Burst capture ensures the best possible frame is selected without requiring photography skill.

Output: Native frame processor plugins (iOS + Android), guidance overlay UI components, burst capture logic, integrated into existing CameraScreen.
</objective>

<execution_context>
@/home/jbellot/.claude/get-shit-done/workflows/execute-plan.md
@/home/jbellot/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-camera-guidance-and-artistic-styles/04-RESEARCH.md
@.planning/phases/02-camera-capture-and-image-upload/02-02-SUMMARY.md
@mobile/src/screens/Camera/CameraScreen.tsx
@mobile/src/components/Camera/IrisGuideOverlay.tsx
@mobile/src/components/Camera/ShutterButton.tsx
@mobile/src/hooks/useCamera.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Native frame processor plugins for iris detection, blur detection, and lighting analysis</name>
  <files>
    mobile/ios/FrameProcessors/IrisDetectionPlugin.swift
    mobile/ios/FrameProcessors/IrisDetectionPlugin.m
    mobile/ios/FrameProcessors/BlurDetectionPlugin.swift
    mobile/ios/FrameProcessors/BlurDetectionPlugin.m
    mobile/ios/FrameProcessors/LightingAnalysisPlugin.swift
    mobile/ios/FrameProcessors/LightingAnalysisPlugin.m
    mobile/android/app/src/main/java/com/irisart/frameprocessors/IrisDetectionPlugin.kt
    mobile/android/app/src/main/java/com/irisart/frameprocessors/BlurDetectionPlugin.kt
    mobile/android/app/src/main/java/com/irisart/frameprocessors/LightingAnalysisPlugin.kt
    mobile/android/app/src/main/java/com/irisart/frameprocessors/FrameProcessorPackage.kt
    mobile/src/frameProcessors/irisDetection.ts
    mobile/src/frameProcessors/blurDetection.ts
    mobile/src/frameProcessors/lightingAnalysis.ts
    mobile/src/hooks/useIrisDetection.ts
  </files>
  <action>
Create native frame processor plugins for Vision Camera that perform on-device iris detection, blur detection, and lighting analysis. These run on camera frames at 30 FPS with frame skipping (process every 2nd-3rd frame) to stay within 33ms budget.

**iOS plugins (Swift + Objective-C bridge):**

1. **IrisDetectionPlugin.swift** + **IrisDetectionPlugin.m** (ObjC bridge):
   - Register as `detectIris` frame processor plugin using `@objc(IrisDetectionPlugin)` and VisionCamera's `FrameProcessorPlugin` base class
   - Use Apple's Vision framework `VNDetectFaceLandmarksRequest` to detect face landmarks and extract eye regions
   - Extract eye center coordinates (left and right eye), compute iris center as midpoint
   - Calculate approximate iris radius from eye region bounds
   - Estimate distance based on eye region size relative to frame (empirical: iris appears larger when phone is closer)
   - Return dictionary: `{ detected: Bool, centerX: Float, centerY: Float, radius: Float, distance: Float }` where distance is normalized 0-1 (0=too far, 0.5=ideal 10-15cm, 1=too close)
   - Performance target: <30ms per frame

2. **BlurDetectionPlugin.swift** + **BlurDetectionPlugin.m**:
   - Register as `detectBlur` frame processor plugin
   - Convert frame to grayscale using CIImage filters
   - Apply Laplacian convolution kernel (3x3: [0,1,0, 1,-4,1, 0,1,0]) using CIConvolution3X3 or Accelerate framework
   - Calculate variance of Laplacian output (higher variance = sharper image)
   - Return: `{ sharpness: Float, isBlurry: Bool }` where sharpness is raw variance and isBlurry = sharpness < 100.0 (threshold tunable)
   - Performance target: <5ms per frame

3. **LightingAnalysisPlugin.swift** + **LightingAnalysisPlugin.m**:
   - Register as `analyzeLighting` frame processor plugin
   - Sample 30 pixels from center region of frame (avoid edges for accuracy)
   - Calculate average luminance from sampled pixels (Y channel from YCbCr or computed from RGB)
   - Return: `{ brightness: Float, status: String }` where brightness is 0-1, status is "too_dark" (< 0.25), "too_bright" (> 0.85), or "good"
   - Performance target: <5ms per frame

**Android plugins (Kotlin):**

4. **IrisDetectionPlugin.kt**:
   - Register as `detectIris` frame processor plugin using VisionCamera's `FrameProcessorPlugin` class
   - Use ML Kit Face Detection with `FaceDetectorOptions.LANDMARK_MODE_ALL` for face landmark detection
   - Extract left/right eye landmarks, compute iris center and radius from eye bounding box
   - Estimate distance from eye region proportion relative to frame size
   - Return same result format as iOS: `{ detected, centerX, centerY, radius, distance }`
   - Important: Use `InputImage.fromMediaImage()` to convert camera frame — DO NOT convert to Bitmap (slow)
   - Performance target: <30ms per frame

5. **BlurDetectionPlugin.kt**:
   - Register as `detectBlur` frame processor plugin
   - Use OpenCV (via org.opencv) or Android RenderScript for Laplacian variance
   - If OpenCV not available, use simple approach: extract Y channel from YUV frame, compute Laplacian manually using convolution kernel, calculate variance
   - Alternative simpler approach: Use Android's RenderScript ScriptIntrinsicConvolve3x3 with Laplacian kernel
   - Return: `{ sharpness, isBlurry }` same format as iOS

6. **LightingAnalysisPlugin.kt**:
   - Register as `analyzeLighting` frame processor plugin
   - Sample center pixels from Y plane of YUV frame (camera frames are typically YUV_420_888)
   - Calculate average luminance
   - Return: `{ brightness, status }` same format as iOS

7. **FrameProcessorPackage.kt**:
   - Register all three plugins with VisionCamera's FrameProcessorPluginRegistry
   - Must be registered in the app's MainApplication or ReactPackage

**TypeScript wrappers:**

8. **mobile/src/frameProcessors/irisDetection.ts**:
   - Import `VisionCameraProxy` from `react-native-vision-camera`
   - Call `VisionCameraProxy.initFrameProcessorPlugin('detectIris')` to get native plugin reference
   - Export `detectIris(frame: Frame)` function marked with `'worklet'` directive
   - Define `IrisDetectionResult` interface: `{ detected: boolean, centerX: number, centerY: number, radius: number, distance: number }`

9. **mobile/src/frameProcessors/blurDetection.ts**:
   - Same pattern: init `detectBlur` plugin
   - Export `detectBlur(frame: Frame)` worklet
   - Define `BlurDetectionResult`: `{ sharpness: number, isBlurry: boolean }`

10. **mobile/src/frameProcessors/lightingAnalysis.ts**:
    - Same pattern: init `analyzeLighting` plugin
    - Export `analyzeLighting(frame: Frame)` worklet
    - Define `LightingAnalysisResult`: `{ brightness: number, status: 'too_dark' | 'too_bright' | 'good' }`

11. **mobile/src/hooks/useIrisDetection.ts**:
    - Custom hook that composes all three frame processor plugins into a single `useFrameProcessor` callback
    - Skip frames: process every 3rd frame to stay within budget (use frame count or timestamp modulo)
    - Use `runOnJS` to bridge worklet results to React state
    - Maintain state: `{ irisDetected, irisCenter, irisRadius, distance, focusQuality, isBlurry, brightness, lightingStatus }`
    - Export `guidanceState` object and `frameProcessor` for CameraScreen to use
    - Include `isReady` boolean (true once first detection completes, prevents flash of "no iris" on startup)
    - Return a `readyToCapture` boolean: iris detected + not blurry + good lighting

**Important notes:**
- For iOS, each plugin needs an Objective-C bridging file (.m) to expose the Swift class to React Native. Use `RCT_EXTERN_MODULE` macro.
- For Android, check the actual package name from the existing android project structure (may be `com.mobile` or similar, not `com.irisart`). Match what exists.
- Reference Vision Camera v4 frame processor plugin documentation for correct base class and registration pattern.
- Do NOT use MediaPipe (too heavy for our single-task need). Use Apple Vision on iOS and ML Kit on Android.
- ML Kit face detection dependency for Android: add `com.google.mlkit:face-detection:16.1.7` to android/app/build.gradle.
  </action>
  <verify>
    - `npx tsc --noEmit` passes (TypeScript wrappers compile)
    - All iOS plugin .swift and .m files exist in mobile/ios/FrameProcessors/
    - All Android plugin .kt files exist in correct package directory under mobile/android/
    - `grep -r "initFrameProcessorPlugin" mobile/src/frameProcessors/` finds 3 plugin registrations
    - `grep -r "useIrisDetection" mobile/src/hooks/` finds the composite hook
    - `grep "face-detection" mobile/android/app/build.gradle` confirms ML Kit dependency added
  </verify>
  <done>
    - Three native frame processor plugins exist for iOS (Swift) and Android (Kotlin)
    - TypeScript wrappers expose detectIris, detectBlur, analyzeLighting as worklet functions
    - useIrisDetection hook composes all three into unified guidance state with frame skipping
    - readyToCapture boolean available for triggering burst capture
  </done>
</task>

<task type="auto">
  <name>Task 2: Camera guidance overlay UI and burst capture with best-frame selection</name>
  <files>
    mobile/src/components/Camera/CameraGuidanceOverlay.tsx
    mobile/src/components/Camera/IrisAlignmentGuide.tsx
    mobile/src/components/Camera/FocusQualityIndicator.tsx
    mobile/src/components/Camera/LightingIndicator.tsx
    mobile/src/components/Camera/BurstCaptureButton.tsx
    mobile/src/screens/Camera/CameraScreen.tsx
    mobile/src/navigation/types.ts
  </files>
  <action>
Build the camera guidance overlay UI components and burst capture logic, then integrate into the existing CameraScreen.

**UI Components:**

1. **IrisAlignmentGuide.tsx** (replace existing IrisGuideOverlay.tsx role):
   - Animated circular guide overlay that reacts to iris detection state
   - When no iris detected: Circle is white/gray, text says "Position your eye in the circle"
   - When iris detected but misaligned: Circle turns yellow, shows directional hint ("Move left", "Move closer", "Move away") based on distance and position
   - When iris aligned and ready: Circle turns green, gentle pulse animation using Reanimated
   - Circle size adjusts based on detected iris radius (proportional feedback)
   - Show distance indicator: "Too far — Move closer" / "Perfect distance" / "Too close — Move away" based on distance value from useIrisDetection (< 0.3 = too far, 0.3-0.7 = good, > 0.7 = too close)
   - Smooth transitions between states using Reanimated shared values and interpolation

2. **FocusQualityIndicator.tsx**:
   - Small icon + text in bottom-left of camera view
   - States: "Sharp" (green checkmark), "Slightly blurry" (yellow warning), "Too blurry" (red X)
   - Map sharpness from useIrisDetection: > 150 = sharp, 80-150 = slightly blurry, < 80 = too blurry
   - Animate color transitions smoothly
   - Only show after iris is detected (don't confuse user before that)

3. **LightingIndicator.tsx**:
   - Small icon + text in bottom-right of camera view
   - States based on lightingStatus from hook: "Good lighting" (green), "Too dark" (amber, "Find more light"), "Too bright" (amber, "Reduce glare")
   - Same smooth animation pattern as FocusQualityIndicator
   - Only show after iris is detected

4. **CameraGuidanceOverlay.tsx**:
   - Composite component that renders IrisAlignmentGuide + FocusQualityIndicator + LightingIndicator
   - Receives guidanceState from useIrisDetection hook as props
   - Absolutely positioned over the camera view (StyleSheet.absoluteFill)
   - Shows "Getting ready..." text briefly before first detection completes (using isReady from hook)

5. **BurstCaptureButton.tsx** (replace or extend existing ShutterButton):
   - Same visual design as existing ShutterButton (large circular white button, 70px)
   - On press: capture 3 frames 200ms apart using `cameraRef.takePhoto()`
   - After capturing 3 frames, analyze each for sharpness (using blur detection on saved image)
   - For sharpness analysis of saved photos: use simple approach — load image, call a lightweight analysis function, or use the last known sharpness from frame processor state for each capture moment
   - Auto-select the frame with highest sharpness score
   - Delete non-selected frames from temp storage using react-native-fs (or platform file API)
   - Show brief "Selecting best shot..." text during analysis (< 1 second)
   - Navigate to PhotoReviewScreen with the best frame
   - Disabled state: button appears dimmed when `readyToCapture` is false (iris not detected, too blurry, or bad lighting)
   - Enabled state: full brightness when readyToCapture is true
   - Haptic feedback on capture (using Vibration API)

**CameraScreen.tsx Updates:**

6. Update existing CameraScreen to integrate guidance:
   - Import and use useIrisDetection hook to get guidanceState and frameProcessor
   - Pass frameProcessor to Camera component's `frameProcessor` prop
   - Render CameraGuidanceOverlay instead of (or in addition to) existing IrisGuideOverlay
   - Replace ShutterButton with BurstCaptureButton
   - Pass readyToCapture to BurstCaptureButton for enabled/disabled state
   - Keep existing controls (flash, camera switch, zoom) — they still work alongside guidance
   - Set camera to 30 FPS explicitly for frame processor performance: `fps={30}`
   - Set camera format to limit resolution for frame processing: prefer 720p or 1080p format for frames, but capture at full resolution

**Navigation types:**
- No changes needed to PhotoReviewScreen params — same photoPath/width/height structure works

**Important implementation details:**
- Use `react-native-reanimated` for all animations (already in project)
- The frame processor runs on the worklet thread — use `runOnJS` only for the state update, not for heavy computation
- BurstCaptureButton should save frames to temp directory (not permanent), delete non-selected frames immediately
- Keep the existing IrisGuideOverlay.tsx file but it will no longer be rendered in CameraScreen (CameraGuidanceOverlay replaces it)
- Test that `readyToCapture` doesn't flicker rapidly — add a 500ms debounce (must stay ready for 500ms before enabling capture button)
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `grep -r "CameraGuidanceOverlay" mobile/src/screens/Camera/CameraScreen.tsx` confirms overlay integrated
    - `grep -r "BurstCaptureButton" mobile/src/screens/Camera/CameraScreen.tsx` confirms burst capture integrated
    - `grep -r "frameProcessor" mobile/src/screens/Camera/CameraScreen.tsx` confirms frame processor wired to camera
    - `grep -r "useIrisDetection" mobile/src/screens/Camera/CameraScreen.tsx` confirms hook usage
    - All 5 new component files exist in mobile/src/components/Camera/
  </verify>
  <done>
    - CameraGuidanceOverlay shows iris alignment guide that reacts to detection (red/yellow/green states)
    - FocusQualityIndicator and LightingIndicator show real-time feedback
    - BurstCaptureButton captures 3 frames and auto-selects best one
    - CameraScreen integrates guidance overlay, frame processor, and burst capture
    - Capture button disabled until iris detected, focused, and well-lit
  </done>
</task>

</tasks>

<verification>
1. TypeScript compilation: `cd mobile && npx tsc --noEmit` passes without errors
2. Native plugin files exist for both iOS and Android platforms
3. Frame processor plugin registration pattern confirmed in TypeScript wrappers
4. CameraScreen renders CameraGuidanceOverlay and BurstCaptureButton
5. useIrisDetection hook composes all three detection plugins with frame skipping
6. All guidance states (no iris, misaligned, ready) have distinct visual feedback
</verification>

<success_criteria>
- Real-time guidance overlay updates on camera view with iris detection, focus quality, and lighting feedback
- Capture button is disabled until conditions are met (iris detected + sharp + good lighting)
- Burst capture takes 3 frames and selects the best one automatically
- Frame processor runs within 33ms budget at 30 FPS (every 3rd frame processed)
- All TypeScript compiles and native plugin boilerplate is correctly structured
</success_criteria>

<output>
After completion, create `.planning/phases/04-camera-guidance-and-artistic-styles/04-01-SUMMARY.md`
</output>
