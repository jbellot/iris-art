---
phase: 01-foundation-and-privacy-architecture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/Dockerfile
  - backend/docker-compose.yml
  - backend/requirements.txt
  - backend/app/__init__.py
  - backend/app/main.py
  - backend/app/core/__init__.py
  - backend/app/core/config.py
  - backend/app/core/db.py
  - backend/app/models/__init__.py
  - backend/app/models/user.py
  - backend/app/models/consent.py
  - backend/app/schemas/__init__.py
  - backend/app/schemas/user.py
  - backend/app/api/__init__.py
  - backend/app/api/deps.py
  - backend/app/api/routes/__init__.py
  - backend/app/api/routes/health.py
  - backend/app/storage/__init__.py
  - backend/app/storage/s3.py
  - backend/app/workers/__init__.py
  - backend/app/workers/celery_app.py
  - backend/alembic.ini
  - backend/alembic/env.py
  - backend/alembic/script.py.mako
autonomous: true

must_haves:
  truths:
    - "Docker Compose starts the full local dev stack (FastAPI, PostgreSQL, Redis, Celery worker, MinIO) with a single command"
    - "FastAPI app responds to HTTP requests with async database access"
    - "Alembic can generate and run migrations against the async PostgreSQL database"
    - "S3 client can upload and retrieve objects from MinIO with server-side encryption"
    - "Celery worker connects to Redis and can execute tasks"
  artifacts:
    - path: "backend/docker-compose.yml"
      provides: "Full local dev stack orchestration"
      contains: "services:"
    - path: "backend/app/main.py"
      provides: "FastAPI application entry point"
      contains: "FastAPI"
    - path: "backend/app/core/db.py"
      provides: "Async SQLAlchemy engine and session factory"
      contains: "async_sessionmaker"
    - path: "backend/app/core/config.py"
      provides: "Pydantic BaseSettings configuration"
      contains: "BaseSettings"
    - path: "backend/app/models/user.py"
      provides: "User SQLAlchemy model"
      contains: "class User"
    - path: "backend/app/models/consent.py"
      provides: "ConsentRecord SQLAlchemy model"
      contains: "class ConsentRecord"
    - path: "backend/app/storage/s3.py"
      provides: "S3 client with encryption"
      contains: "ServerSideEncryption"
    - path: "backend/app/workers/celery_app.py"
      provides: "Celery instance with Redis broker"
      contains: "Celery"
    - path: "backend/alembic/env.py"
      provides: "Async Alembic migration environment"
      contains: "run_async_migrations"
  key_links:
    - from: "backend/app/core/db.py"
      to: "backend/app/core/config.py"
      via: "DATABASE_URL from settings"
      pattern: "settings\\.database_url"
    - from: "backend/app/api/deps.py"
      to: "backend/app/core/db.py"
      via: "get_session dependency"
      pattern: "async_session_maker"
    - from: "backend/docker-compose.yml"
      to: "backend/Dockerfile"
      via: "build context"
      pattern: "build:"
    - from: "backend/app/workers/celery_app.py"
      to: "backend/app/core/config.py"
      via: "Redis broker URL from settings"
      pattern: "celery_broker_url"
---

<objective>
Scaffold the entire backend foundation: FastAPI app with async SQLAlchemy 2.0, PostgreSQL, Redis, Celery workers, MinIO (S3-compatible storage), and Docker Compose for one-command local development.

Purpose: This is the foundational layer everything else builds on. Auth (Plan 02) needs the User model, DB sessions, and Redis. Privacy (Plan 03) needs S3 storage and the ConsentRecord model. All future phases depend on this infrastructure.

Output: A working Docker Compose stack where `docker compose up` starts FastAPI (port 8000), PostgreSQL (5432), Redis (6379), Celery worker, Flower (5555), and MinIO (9000/9001). FastAPI serves a health endpoint, Alembic can run migrations, and the S3 client can upload encrypted objects.
</objective>

<execution_context>
@/home/jbellot/.claude/get-shit-done/workflows/execute-plan.md
@/home/jbellot/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-and-privacy-architecture/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure, configuration, database layer, and models</name>
  <files>
    backend/app/__init__.py
    backend/app/main.py
    backend/app/core/__init__.py
    backend/app/core/config.py
    backend/app/core/db.py
    backend/app/models/__init__.py
    backend/app/models/user.py
    backend/app/models/consent.py
    backend/app/schemas/__init__.py
    backend/app/schemas/user.py
    backend/app/api/__init__.py
    backend/app/api/deps.py
    backend/app/api/routes/__init__.py
    backend/app/api/routes/health.py
    backend/app/services/__init__.py
    backend/app/storage/__init__.py
    backend/app/storage/s3.py
    backend/app/workers/__init__.py
    backend/app/workers/celery_app.py
    backend/app/workers/tasks/__init__.py
    backend/requirements.txt
    backend/alembic.ini
    backend/alembic/env.py
    backend/alembic/script.py.mako
    backend/alembic/versions/.gitkeep
  </files>
  <action>
    Create the full backend project structure following the research-recommended layout.

    **backend/app/core/config.py:** Pydantic BaseSettings class loading from environment variables:
    - DATABASE_URL (postgresql+asyncpg://...)
    - CELERY_BROKER_URL (redis://...)
    - CELERY_RESULT_BACKEND (redis://...)
    - S3_ENDPOINT, S3_ACCESS_KEY, S3_SECRET_KEY, S3_BUCKET_NAME
    - SECRET_KEY (for JWT, generate default with secrets.token_hex(32))
    - ACCESS_TOKEN_EXPIRE_MINUTES (default 30)
    - REFRESH_TOKEN_EXPIRE_DAYS (default 30)
    - APP_NAME (default "IrisVue")
    - DEBUG (default False)
    Use `model_config = SettingsConfigDict(env_file=".env")` for .env file support.

    **backend/app/core/db.py:** Async SQLAlchemy setup:
    - create_async_engine with DATABASE_URL from settings
    - async_sessionmaker with expire_on_commit=False (CRITICAL: prevents lazy load errors in async)
    - DeclarativeBase class (Base) for all models to inherit from
    - get_session() async generator for dependency injection

    **backend/app/models/user.py:** User model with:
    - id: UUID primary key (use uuid7 via Python uuid for time-ordered UUIDs, or uuid4)
    - email: String, unique, indexed, not nullable
    - hashed_password: String, nullable (null for OAuth-only users)
    - is_active: Boolean, default True
    - is_verified: Boolean, default False (email verification)
    - auth_provider: String, nullable (apple, google, or null for email)
    - auth_provider_id: String, nullable (external provider user ID)
    - created_at: DateTime with timezone, server_default=func.now()
    - updated_at: DateTime with timezone, onupdate=func.now()
    Add relationship to ConsentRecord (one-to-many).

    **backend/app/models/consent.py:** ConsentRecord model with:
    - id: UUID primary key
    - user_id: UUID, ForeignKey to users.id, not nullable
    - consent_type: String (biometric_capture, data_processing, marketing)
    - jurisdiction: String (gdpr, bipa, ccpa, generic)
    - granted: Boolean, not nullable
    - granted_at: DateTime with timezone
    - withdrawn_at: DateTime with timezone, nullable
    - ip_address: String (for audit trail)
    - consent_text_version: String (tracks which text was shown)
    - created_at: DateTime with timezone, server_default=func.now()

    **backend/app/models/__init__.py:** Import Base, User, ConsentRecord so Alembic discovers them.

    **backend/app/schemas/user.py:** Pydantic schemas:
    - UserRead (id, email, is_active, is_verified, created_at)
    - UserCreate (email, password with min length 8)

    **backend/app/api/deps.py:** Shared dependencies:
    - get_session: yields AsyncSession from async_session_maker
    - (Placeholder for get_current_user -- implemented in Plan 02)

    **backend/app/api/routes/health.py:** Health check endpoint:
    - GET /health returns {"status": "healthy", "version": "0.1.0"}
    - GET /health/db tests database connection (execute SELECT 1)
    - GET /health/redis tests Redis connection

    **backend/app/main.py:** FastAPI app:
    - Create FastAPI instance with title="IrisVue API"
    - Include health router
    - Add CORS middleware (allow_origins configurable, NOT ["*"] -- use settings)
    - Add lifespan handler for startup/shutdown (test DB connection on startup)

    **backend/app/storage/s3.py:** S3 client:
    - Initialize boto3 client from settings (endpoint_url, access_key, secret_key)
    - upload_file(bucket, key, data, content_type) with ServerSideEncryption='AES256'
    - download_file(bucket, key) returns bytes
    - generate_presigned_url(bucket, key, expiry=3600) for temporary access
    - delete_file(bucket, key)
    - delete_user_files(bucket, user_id_prefix) for GDPR deletion
    - ensure_bucket(bucket) creates bucket if not exists (for dev setup)

    **backend/app/workers/celery_app.py:** Celery instance:
    - Create Celery app named "iris_art" with broker and backend from settings
    - Configure task_routes for email and export queues
    - Set task_serializer='json', accept_content=['json']
    - Configure task_track_started=True for progress tracking

    **backend/requirements.txt:** Pin all dependencies:
    ```
    fastapi[standard]>=0.115.0
    sqlalchemy[asyncio]>=2.0.0
    asyncpg>=0.29.0
    alembic>=1.13.0
    redis>=4.5.4
    celery>=5.2.7
    pyjwt>=2.8.0
    pwdlib[argon2]
    authlib>=1.6.6
    itsdangerous
    python-multipart
    boto3
    uvicorn[standard]
    flower
    pydantic-settings
    httpx
    geoip2
    cryptography
    ```

    **backend/alembic.ini:** Standard Alembic config pointing to backend/alembic for script_location, with sqlalchemy.url placeholder (overridden in env.py).

    **backend/alembic/env.py:** Async Alembic environment:
    - Import Base and all models from app.models
    - Import settings for DATABASE_URL
    - Implement run_async_migrations() using async engine
    - Set target_metadata = Base.metadata
    Use the pattern from SQLAlchemy docs for async Alembic (create_async_engine in env.py, run_sync for migration context).

    **backend/alembic/script.py.mako:** Standard Alembic migration template.

    AVOID:
    - Do NOT use lazy loading relationships (always define relationship with back_populates, use selectinload in queries)
    - Do NOT use session.query() API (use select() statements -- SQLAlchemy 2.0 pattern)
    - Do NOT hardcode secrets (all from environment/settings)
    - Do NOT use tiangolo/uvicorn-gunicorn-fastapi Docker image (deprecated)
  </action>
  <verify>
    Run: `cd /home/jbellot/Documents/repo/iris-art/backend && python -c "from app.core.config import settings; print(settings.APP_NAME)"` should print "IrisVue".
    Run: `cd /home/jbellot/Documents/repo/iris-art/backend && python -c "from app.models import Base, User, ConsentRecord; print(Base.metadata.tables.keys())"` should list users and consent_records tables.
    Verify all __init__.py files exist and imports work without errors.
  </verify>
  <done>
    All Python source files exist with correct imports. Settings load from environment. User and ConsentRecord models are defined with all specified fields. S3 client has upload/download/delete operations with encryption. Celery app is configured. Alembic env.py uses async engine. Health route exists.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Docker Compose stack and verify full local environment</name>
  <files>
    backend/Dockerfile
    backend/docker-compose.yml
    backend/.env.example
    backend/.dockerignore
  </files>
  <action>
    **backend/Dockerfile:** Multi-stage build:
    - Base image: python:3.12-slim
    - Set WORKDIR /code
    - Copy requirements.txt and pip install (use --no-cache-dir)
    - Copy app/ directory
    - Expose 8000
    - CMD: uvicorn app.main:app --host 0.0.0.0 --port 8000

    **backend/docker-compose.yml:** Full local dev stack:
    - **web** service: builds from Dockerfile, command overrides to `uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload`, volumes mount ./app:/code/app for hot reload, ports 8000:8000, env from .env file, depends_on db (service_healthy) and redis (service_started)
    - **db** service: postgres:15, env vars POSTGRES_USER/PASSWORD/DB, port 5432:5432, volume postgres_data, healthcheck with pg_isready
    - **redis** service: redis:7-alpine, port 6379:6379
    - **celery_worker** service: same build as web, command `celery -A app.workers.celery_app worker --loglevel=info`, volumes mount ./app, same env vars, depends_on db and redis
    - **flower** service: same build, command `celery -A app.workers.celery_app flower --port=5555`, port 5555:5555, depends_on redis and celery_worker
    - **minio** service: minio/minio, command `server /data --console-address ":9001"`, ports 9000:9000 and 9001:9001, env MINIO_ROOT_USER/PASSWORD, volume minio_data
    - Volumes: postgres_data, minio_data

    **backend/.env.example:** Template with all required environment variables and development defaults:
    ```
    DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/iris_art
    CELERY_BROKER_URL=redis://redis:6379/0
    CELERY_RESULT_BACKEND=redis://redis:6379/0
    S3_ENDPOINT=http://minio:9000
    S3_ACCESS_KEY=minioadmin
    S3_SECRET_KEY=minioadmin
    S3_BUCKET_NAME=iris-art
    SECRET_KEY=dev-secret-change-in-production
    DEBUG=true
    CORS_ORIGINS=["http://localhost:3000","http://localhost:8081"]
    ```

    Also copy .env.example to .env for immediate dev use.

    **backend/.dockerignore:** Exclude .git, __pycache__, .env, .venv, *.pyc, node_modules.

    After creating files, run:
    1. `cd /home/jbellot/Documents/repo/iris-art/backend && docker compose up -d --build`
    2. Wait for services to be healthy
    3. Run initial Alembic migration: `docker compose exec web alembic revision --autogenerate -m "initial_models"` then `docker compose exec web alembic upgrade head`
    4. Test health endpoint: `curl http://localhost:8000/health`
    5. Test DB health: `curl http://localhost:8000/health/db`
    6. Verify MinIO is accessible: `curl -s http://localhost:9001` (should return HTML)
    7. Create the S3 bucket via the web service: `docker compose exec web python -c "from app.storage.s3 import ensure_bucket; ensure_bucket('iris-art')"`
    8. Run `docker compose down` to clean up (leave volumes for next run)

    AVOID:
    - Do NOT use `version: '3.8'` in docker-compose.yml (deprecated in Compose V2)
    - Do NOT use tiangolo base images
    - Do NOT expose database ports in production (fine for dev)
  </action>
  <verify>
    Run: `cd /home/jbellot/Documents/repo/iris-art/backend && docker compose up -d --build` should start all 6 services without errors.
    Run: `curl http://localhost:8000/health` should return `{"status":"healthy","version":"0.1.0"}`.
    Run: `curl http://localhost:8000/health/db` should return success (database connected).
    Run: `docker compose exec web alembic current` should show the migration head.
    Run: `docker compose ps` should show all services as running/healthy.
    Run: `docker compose down` to clean up.
  </verify>
  <done>
    Docker Compose starts the full stack (FastAPI, PostgreSQL, Redis, Celery, Flower, MinIO) with `docker compose up`. Health endpoints respond. Alembic migrations run against the database. MinIO bucket exists. All services communicate correctly.
  </done>
</task>

</tasks>

<verification>
1. `docker compose up -d` starts all 6 services (web, db, redis, celery_worker, flower, minio)
2. `curl http://localhost:8000/health` returns 200 with healthy status
3. `curl http://localhost:8000/health/db` returns 200 (database connected)
4. `docker compose exec web alembic current` shows applied migration
5. `docker compose exec web python -c "from app.models import User, ConsentRecord; print('Models OK')"` succeeds
6. MinIO console accessible at http://localhost:9001
7. Flower dashboard accessible at http://localhost:5555
</verification>

<success_criteria>
- Docker Compose starts the full local dev stack with a single command
- FastAPI health endpoint responds on port 8000
- PostgreSQL has User and ConsentRecord tables via Alembic migration
- Redis is connected (health check passes)
- Celery worker is running and connected to Redis broker
- MinIO S3-compatible storage is running with iris-art bucket
- All Python imports work without errors
- No hardcoded secrets (all from environment/settings)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-and-privacy-architecture/01-01-SUMMARY.md`
</output>
